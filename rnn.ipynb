{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca725f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T03:06:08.343602Z",
     "iopub.status.busy": "2023-05-05T03:06:08.342752Z",
     "iopub.status.idle": "2023-05-05T03:06:08.374060Z",
     "shell.execute_reply": "2023-05-05T03:06:08.372874Z"
    },
    "papermill": {
     "duration": 0.040123,
     "end_time": "2023-05-05T03:06:08.376968",
     "exception": false,
     "start_time": "2023-05-05T03:06:08.336845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c933b44",
   "metadata": {},
   "source": [
    "Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27c67650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T03:06:08.395833Z",
     "iopub.status.busy": "2023-05-05T03:06:08.395034Z",
     "iopub.status.idle": "2023-05-05T03:06:08.407887Z",
     "shell.execute_reply": "2023-05-05T03:06:08.406484Z"
    },
    "papermill": {
     "duration": 0.02136,
     "end_time": "2023-05-05T03:06:08.410841",
     "exception": false,
     "start_time": "2023-05-05T03:06:08.389481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "        \n",
    "        self.chars = list(set(data))\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "  \n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # convert to lowercase, removing leading/trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    " \n",
    "    def generate_example(self, idx):\n",
    "       \n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "        \n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "        \n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db021c65",
   "metadata": {},
   "source": [
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c3c21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T03:06:08.446359Z",
     "iopub.status.busy": "2023-05-05T03:06:08.445929Z",
     "iopub.status.idle": "2023-05-05T03:06:08.498152Z",
     "shell.execute_reply": "2023-05-05T03:06:08.496999Z"
    },
    "papermill": {
     "duration": 0.061654,
     "end_time": "2023-05-05T03:06:08.501229",
     "exception": false,
     "start_time": "2023-05-05T03:06:08.439575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, hidden_size, data_generator, sequence_length, learning_rate):\n",
    "       \n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.data_generator = data_generator\n",
    "        self.vocab_size = self.data_generator.vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = np.random.uniform(-np.sqrt(1. / self.vocab_size), np.sqrt(1. / self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.Waa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (hidden_size, hidden_size))\n",
    "        self.Wya = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))  \n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        \n",
    "        # parameter update with AdamW\n",
    "        self.mWax = np.zeros_like(self.Wax)\n",
    "        self.vWax = np.zeros_like(self.Wax)\n",
    "        self.mWaa = np.zeros_like(self.Waa)\n",
    "        self.vWaa = np.zeros_like(self.Waa)\n",
    "        self.mWya = np.zeros_like(self.Wya)\n",
    "        self.vWya = np.zeros_like(self.Wya)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Compute the softmax activation fn\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow\n",
    "        x = x - np.max(x)\n",
    "        p = np.exp(x)\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        \"\"\"\n",
    "        Forward pass of the RNN\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize dictionaries to store activations and output probabilities.\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "\n",
    "        self.X = X\n",
    "        a[-1] = np.copy(a_prev)\n",
    "       \n",
    "        for t in range(len(self.X)): \n",
    "           \n",
    "            x[t] = np.zeros((self.vocab_size,1)) \n",
    "            if (self.X[t] != None):\n",
    "                x[t][self.X[t]] = 1\n",
    "       \n",
    "            a[t] = np.tanh(np.dot(self.Wax, x[t]) + np.dot(self.Waa, a[t - 1]) + self.ba)\n",
    "    \n",
    "            y_pred[t] = self.softmax(np.dot(self.Wya, a[t]) + self.by)\n",
    "       \n",
    "        return x, a, y_pred \n",
    "    \n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Backward pass of the RNN\n",
    "\n",
    "        \"\"\"\n",
    "        da_next = np.zeros_like(a[0])\n",
    "\n",
    "        for t in reversed(range(len(self.X))):\n",
    " \n",
    "            dy_preds = np.copy(y_preds[t])\n",
    "            dy_preds[targets[t]] -= 1\n",
    "\n",
    "            da = np.dot(self.Waa.T, da_next) + np.dot(self.Wya.T, dy_preds)\n",
    "            dtanh = (1 - np.power(a[t], 2))\n",
    "            da_unactivated = dtanh * da\n",
    "\n",
    "            # Calculate gradients\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += np.dot(da_unactivated, x[t].T)\n",
    "            self.dWaa += np.dot(da_unactivated, a[t - 1].T)\n",
    "\n",
    "            da_next = da_unactivated\n",
    "\n",
    "            self.dWya += np.dot(dy_preds, a[t].T)\n",
    "\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                np.clip(grad, -1, 1, out=grad)\n",
    " \n",
    "    def loss(self, y_preds, targets):\n",
    "        \n",
    "        # cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(len(self.X)))\n",
    "    \n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * np.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * np.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * np.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "    \n",
    "    def sample(self):\n",
    "       \n",
    "        # initialize input and hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        indices = []\n",
    "        idx = -1\n",
    "\n",
    "        counter = 0\n",
    "        max_chars = 50 \n",
    "        newline_character = self.data_generator.char_to_idx['\\n']\n",
    "\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "      \n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "\n",
    "            y = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "\n",
    "            idx = np.random.choice(list(range(self.vocab_size)), p=y.ravel())\n",
    "\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            indices.append(idx)\n",
    "            a_prev = a\n",
    "            counter += 1\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def train(self, generated_names=5):\n",
    "        \"\"\"\n",
    "        training the RNN using backpropagation through time (BPTT)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 \n",
    "        smooth_loss = -np.log(1.0 / self.data_generator.vocab_size) * self.sequence_length \n",
    "\n",
    "        while (smooth_loss > threshold):\n",
    "            a_prev = np.zeros((self.hidden_size, 1))\n",
    "            idx = iter_num % self.vocab_size\n",
    "            \n",
    "            inputs, targets = self.data_generator.generate_example(idx)\n",
    "\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "       \n",
    "            if iter_num % 500 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                for i in range(generated_names):\n",
    "                    sample_idx = self.sample()\n",
    "                    txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                    txt = txt.title()  \n",
    "                    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "    \n",
    "    def predict(self, start):\n",
    "       \n",
    "\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = self.data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "\n",
    "        max_chars = 50  \n",
    "        newline_character = self.data_generator.char_to_idx['\\n']  \n",
    "        counter = 0\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            \n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "            y_pred = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            a_prev = a\n",
    "            idxes.append(idx)\n",
    "            counter += 1\n",
    "\n",
    "        txt = ''.join(self.data_generator.idx_to_char[i] for i in idxes)\n",
    "\n",
    "        if txt[-1] == '\\n':\n",
    "            txt = txt[:-1]\n",
    "\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825c55a",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-05-05T03:06:08.511984Z",
     "iopub.status.busy": "2023-05-05T03:06:08.511593Z",
     "iopub.status.idle": "2023-05-05T03:07:40.086082Z",
     "shell.execute_reply": "2023-05-05T03:07:40.084273Z"
    },
    "papermill": {
     "duration": 91.585012,
     "end_time": "2023-05-05T03:07:40.090790",
     "exception": false,
     "start_time": "2023-05-05T03:06:08.505778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:82.359906\n",
      "\n",
      "TcyrwlosvhezltamoscqgnqyjnzsvuotqmtizrvacuzjwkrfmzBcnsklwmzspgzgzxkunhytlanoglqnfyxvfrdezxhbsgrbrvghRydlhxcizumyhmvpfvaarjxogwauiafjlby\n",
      "Zauge\n",
      "Rwmhqxrbqlmrtwqwawksnhfpmxaspjblqkmawweyfzwbfpxf\n",
      "\n",
      "\n",
      "iter :500, loss:58.771056\n",
      "\n",
      "Caclvurus\n",
      "Aldoril\n",
      "Ecooloopothomusts\n",
      "Adcaeivphushuhus\n",
      "Adtasossisas\n",
      "\n",
      "\n",
      "iter :1000, loss:41.125384\n",
      "\n",
      "Orydantos\n",
      "Ndenttoosaurus\n",
      "Acaernyxbfrosaurus\n",
      "Mepaosaurus\n",
      "Aprorenttol\n",
      "\n",
      "\n",
      "iter :1500, loss:28.826153\n",
      "\n",
      "Paydonyx\n",
      "Tconcholisaurus\n",
      "Depyptonahorus\n",
      "Abyitdosaurus\n",
      "Pygystosaurus\n",
      "\n",
      "\n",
      "iter :2000, loss:20.430112\n",
      "\n",
      "Aryionyx\n",
      "Achilenethos\n",
      "Aaridooappomimus\n",
      "Afhilousaurus\n",
      "Achillosaurus\n",
      "\n",
      "\n",
      "iter :2500, loss:14.764765\n",
      "\n",
      "Achillobator\n",
      "Aaadonyx\n",
      "Actecaurus\n",
      "Aeolosaurus\n",
      "Acanthol\n",
      "\n",
      "\n",
      "iter :3000, loss:11.060713\n",
      "\n",
      "Actoosaurus\n",
      "Afrovonator\n",
      "Aetonyxafromimus\n",
      "Acanthopholis\n",
      "Aeolosaurus\n",
      "\n",
      "\n",
      "iter :3500, loss:8.702161\n",
      "\n",
      "Afrovenator\n",
      "Actooausaurus\n",
      "Acanthopholis\n",
      "Acalanausaurus\n",
      "Acrollobator\n",
      "\n",
      "\n",
      "iter :4000, loss:7.138212\n",
      "\n",
      "Aetyornithomimus\n",
      "Acritholus\n",
      "Adelolophus\n",
      "Aetiosaurus\n",
      "Aordonyx\n",
      "\n",
      "\n",
      "iter :4500, loss:6.171381\n",
      "\n",
      "Aathelobhimiousaurus\n",
      "Acrottosaurus\n",
      "Aeolosaurus\n",
      "Actiosaurus\n",
      "Abelolophesourus\n",
      "\n",
      "\n",
      "iter :5000, loss:5.533340\n",
      "\n",
      "Aathenosaurus\n",
      "Sardenosaurus\n",
      "Asrdonyx\n",
      "Adeopyxafromemomus\n",
      "Acristavus\n",
      "\n",
      "\n",
      "iter :5500, loss:5.098701\n",
      "\n",
      "Abyillaphus\n",
      "Acrichelus\n",
      "Adelolaurus\n",
      "Achillobator\n",
      "Aordornator\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator('C:/Users/aishi/OneDrive/Desktop/Github/dinos.txt')\n",
    "rnn = RNN(hidden_size=200,data_generator=data_generator, sequence_length=25, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe89bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T03:07:40.119411Z",
     "iopub.status.busy": "2023-05-05T03:07:40.118675Z",
     "iopub.status.idle": "2023-05-05T03:07:40.142872Z",
     "shell.execute_reply": "2023-05-05T03:07:40.141076Z"
    },
    "papermill": {
     "duration": 0.044175,
     "end_time": "2023-05-05T03:07:40.148002",
     "exception": false,
     "start_time": "2023-05-05T03:07:40.103827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meon'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"meo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39019511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-05T03:07:40.178239Z",
     "iopub.status.busy": "2023-05-05T03:07:40.177506Z",
     "iopub.status.idle": "2023-05-05T03:07:40.196826Z",
     "shell.execute_reply": "2023-05-05T03:07:40.195123Z"
    },
    "papermill": {
     "duration": 0.042659,
     "end_time": "2023-05-05T03:07:40.204625",
     "exception": false,
     "start_time": "2023-05-05T03:07:40.161966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alydalaatiomamus'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.927761,
   "end_time": "2023-05-05T03:07:40.525732",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-05T03:05:57.597971",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
